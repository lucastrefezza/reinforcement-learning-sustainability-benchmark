\section{Conclusions}
\label{sec:conclusions}
In this final section, we synthesize the findings and propose next steps.

\subsection{Summary of Findings}
\label{subsec:summary_of_findings}

\begin{itemize}
	\item \textbf{Baseline DQN} attains moderate performance across most environments, 
	emitting $\sim0.0065$\,kg\,CO\textsubscript{2} on average over 100k steps. 
	DoubleDQN reduces Q-value overestimation but does not yield a clear improvement 
	in final returns at this short horizon.
	\item \textbf{PER} overhead raises emissions slightly 
	($\sim0.00725$\,kg\,CO\textsubscript{2}), 
	yet the 100k-step limit masks much of its usual advantage in accelerating learning.
	\item \textbf{DuelingDQN} and \textbf{C51} each show environment-specific benefits 
	(e.g., \emph{Boxing}, \emph{Freeway}), but their aggregated returns 
	remain similar to or slightly exceeding baseline DQN, 
	with comparable runtime/emissions.
	\item \textbf{PPO} stands out as the most “eco-friendly,” completing runs in only 
	2.4\,hours for all seeds/games combined and emitting a mere 0.0029\,kg CO\textsubscript{2}. 
	It achieves respectable performance across many environments.
	\item \textbf{SAC} occasionally dominates a few tasks (\emph{Breakout}) but 
	exhibits the highest energy cost (0.015\,kg\,CO\textsubscript{2}), 
	slow throughput (80--100\,SPS), and large variance in returns.
\end{itemize}

\subsection{Final Thoughts on Energy-Efficient Reinforcement Learning}
\label{subsec:final_thoughts_energy_eff}

The tension between \emph{advanced techniques} and \emph{computational overhead} emerges 
as a core challenge in DRL. Under a short 100k-step regime, simpler methods (PPO, 
baseline DQN) often provide a better ratio of performance to carbon cost, while 
more advanced algorithms, like SAC or distributional DQN variants, struggle to gain a decisive edge 
before the run ends.

In realistic production or industry settings with limited time/budget for model tuning, 
prioritizing algorithms that quickly converge to adequate performance can yield 
significant energy savings. On the other hand, high-level research or competitive RL 
benchmarks often push beyond millions of steps, letting advanced methods eventually 
surpass simpler ones in raw returns.

\subsection{Future Research Directions}
\label{subsec:future_research}

Several pathways could extend this project:

\paragraph{Longer Training Horizons}
Repeating these experiments at 1--5 million steps would clarify how advanced DQN tweaks 
or policy gradient expansions (Rainbow, TD3, etc.) surpass simpler baselines given more time. 
One might track the exact moment (in interactions) at which these enhancements repay 
their higher emissions.

\paragraph{Wider Environment Coverage}
Our 8 chosen games reasonably sample different Atari mechanics, but including 
the full 26 Atari 100k or 55+ ALE tasks 
could reveal whether certain algorithms generalize better to more varied or obscure games.

\paragraph{Real-Time Emissions Minimization}
One future direction might involve \emph{dynamic resource scheduling} or 
\emph{carbon-aware training}, adjusting GPU usage or frequency if real-time 
energy prices or carbon intensities fluctuate throughout the day. 
This idea merges RL with HPC resource management for a truly "green AI".

\paragraph{CPU–GPU Profiling}
A more robust toolchain for CPU and RAM tracking
would help produce more accurate system-level carbon footprints, 
especially for on-policy methods that rely heavily on CPU-based data collection.

\paragraph{RL for LLMs Focus}
In reinforcement learning for LLMs the algorithm choice can have significant sustainability implications. Our results, which confirm that PPO consistently achieves lower carbon emissions and shorter training times even compared to simpler methods, suggest that PPO's dominance in RL fine-tuning might not only be justified by its robust performance but also by its favorable energy efficiency. Nonetheless, emerging proposals such as GRPO, or those discussed in~\cite{ahmadian:back_to_basics}, indicate that even simpler RL approaches might eventually be competitive. A systematic comparison of these methods in the context of LLM fine-tuning is warranted to determine whether further reductions in energy consumption can be achieved without sacrificing performance.

With these possible directions in mind, we hope that the insights gained from 
our 100k-step experiments can foster a broader conversation on 
\emph{energy-efficient deep RL}, balancing performance with real-world sustainability needs.
