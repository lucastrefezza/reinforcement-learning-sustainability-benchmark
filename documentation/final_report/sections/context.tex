\section{Context}
\label{sec:context}
This project addresses the energy consumption of deep reinforcement learning (DRL) solutions and their impact on the environment and business costs.

Beginning with the resurgence of the field following DeepMind's development of \emph{Deep Q-Networks} (DQN) in the early 2010s~\cite{mnih:atari}\cite{mnih:human}, numerous algorithms have been proposed, either by introducing minor modifications to DQN or by adopting entirely different paradigms (such as policy gradient methods), aiming to improve the performance of the learning agents.

Although the performance of the various solutions has been extensively studied, little effort has been directed toward understanding how modifications, whether through tweaks to DQN or entirely different approaches, affect energy consumption, and how these costs compare to those of earlier methods.
This project aims to fill this gap by evaluating the trade-offs between performance and energy consumption across several widely used deep reinforcement learning algorithms, providing valuable insights. Understanding these trade-offs is crucial for businesses and researchers who aim to optimize both performance and sustainability in their applications.

To reach this goal we train various reinforcement learning algorithms on the same task, the choice of which is discussed in section~\vref{subsec:task_selection}. Section~\vref{subsec:algorithm_selection} describes the selected algorithms, whose choice was made taking into account that DRL algorithms can be devided in two main categories: \textit{value based} (i.e. algorithms based on the approximation of a value function, be it the state-value function or the action-value function) and \textit{policy gradient}. The latter are methods that approximate directly the policy, and includes as a special case the \textit{actor-critic methods}, which approximate simultaneously a policy (said actor) and a value function (said critic).