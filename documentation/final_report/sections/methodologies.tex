\section[Methodological Steps]{Methodological Steps}
\label{sec:methodologies}

The methodology used follows from the basic idea of this benchmark: to execute all the algorithms for the same number of environment interactions, so that we can compare the score they achieve and the energy consumption of each one of them. Additionally, a good comparison would be to take the score obtained by the lowest performer in this initial trial and re-train all the algorithms until they reach that score. This would allow us to compare how much time and energy each algorithm requires to achieve the same performance level. Unfortunately, time and resources constraints make retraining all algorithms unfeasible, so we will approximate this second comparison by using the returns from the logging of the training during the first trial. This logging includes the \verb*|global_step|, indicating the environment interaction we are at, and the \verb*|episodic_return|, which is the return of the episode (i.e., the score on which to compare), as well as all performance and power consumption data up to that point. By analyzing these logs, we will estimate how much time and energy each algorithm would take to reach the score obtained by the lowest performer in the initial trial. 

The following sections outline the several key steps involved in the methodology adopted for this project.

\subsection{Algorithms Selection}
\label{subsec:algorithm_selection}
%In our benchmark we will consider, regardind the first category, the DQN, which constitutes the first example of success of deep reinforcement learning (so that we have a sort of baseline), and RAINBOW, a method that involves a lot of the tweaks and improvement made to the original DQN. In addition to these two, we will test various of the single tweaks to assess their individual contribution to energy consumption and performance, and more advanced methods like SPR (Self-Predictive Representations, introduced in the fifth work cited).
%
%Regarding policy gradient and actor critic methods, we will start with a basic one like REINFORCE and/or REINFORCE with baseline (chapter 13 of the first cited work) or the very similar Vanilla Policy Gradient (VPG). We will then move on to Proximal Policy Optimization (PPO), Deep Deterministic Policy Gradient (DDPG)  and its evolutions Twin Delayed DDPG (TD3, seventh work cited) and DRQ (Data-regularized Q, fourth cited work).

As stated, in our benchmark we consider both value-based methods and policy gradient methods. The selected algorithms are chosen to represent a wide range of approaches within both categories.

\subsubsection{Value-Based Methods}

Value-based methods are algorithms based on the approximation of a value function. The following algorithms were considered in this category (but due to time and computational constraints, only a subset of 5 of them were fully trained and evaluated):

\begin{itemize}
	\item \textit{Deep Q-Network (DQN)}: the first example of success in deep reinforcement learning, will serve as a sort of baseline for our benchmark.
	\item \textit{RAINBOW}~\cite{hessel:rainbow}: an advanced method that combines several improvements to the original DQN, that will also be tested individually to assess their individual contributions to energy consumption and performance. These are listed hereafter:
	\begin{itemize}
		\item Double Q-Learning (Double DQN)~\cite{van:double_q};
		\item Prioritized Experience Replay~\cite{schaul:prioritized};
		\item Dueling Network Architectures~\cite{wang:dueling};
		\item Multi-step / N-step Learning~\cite{peng:incremental};
		\item Distributional RL~\cite{bellemare:distributional};
		\item Noisy Nets~\cite{fortunato:noisy};
	\end{itemize}
	\item \textit{Self-Predictive Representations (SPR)}~\cite{schwarzer:spr}: a more advanced method introduced in recent research, which leverages self-predictive representations to improve efficiency.
\end{itemize}

\subsubsection{Policy Gradient Methods}

Policy gradient methods approximate the policy directly and include as a special case the actor-critic methods, which simultaneously approximate a policy and a value function. The algorithms considered in this category are:
\begin{itemize}
	\item \textit{REINFORCE}~\cite[Chapter~13]{sutton:rl}: a basic policy gradient method, or its variant REINFORCE with baseline (also known as Vanilla Policy Gradient, VPG).
	\item \textit{Proximal Policy Optimization (PPO)}~\cite{schulman:ppo}: a popular and efficient policy gradient method that uses a clipped objective to improve training stability.
	\item \textit{Deep Deterministic Policy Gradient (DDPG)}~\cite{lillicrap:ddpg}: an algorithm that combines policy gradients with deterministic policy updates for continuous action spaces.
	\item \textit{Twin Delayed DDPG (TD3)~}\cite{fujimoto:td3}: an improvement over DDPG that addresses function approximation errors through various techniques, such as delayed policy updates and target policy smoothing.
	\item \textit{Soft Actor-Critic (SAC)}~\cite{haarnoja:sac}: an extension of DDPG that incorporates entropy regularization to encourage exploration. SAC, like TD3, uses two Q-networks to reduce overestimation bias, but it differs by optimizing a stochastic policy instead of a deterministic one. This makes SAC more sample-efficient and stable in continuous control tasks. It is also more easily adapted to discrete action spaces.
	\item \textit{Data-Regularized Q (DRQ)}~\cite{kostrikov:drq}: a method that incorporates data augmentation to regularize the training of Q functions, improving performance and stability.
\end{itemize}


\subsection{Task Selection}
\label{subsec:task_selection}

Regarding the task on which to compare the algorithms, there were several suitable candidates: Atari 100k~\cite{kaiser:atari100k}, one of the continuous control task of the DeepMind Control Suite, or one of the many other task (besides Atari) included in OpenAI Gymnasium (formerly Gym), and so on. After various tests and research we opted for the Atari 100k benchmark, a discrete task that consists of playing selected Atari games for only \num{100000} environment interactions.

The reason for this choice is multifaceted. Atari 100k is a widely used benchmark in the DRL community, the wealth of prior research and baseline results available facilitates a more straightforward validation and comparison of our experimental results with those from other studies and algorithms. It is also well suited for evaluating the performance of almost all popular DRL algorithms, ensuring a comprehensive assessment. Additionally, Atari games provide a range of different challenges, including planning, reaction time, and strategy, making it a robust benchmark for assessing general DRL capabilities.

Moreover, the discrete nature of Atari 100k simplifies the implementation and comparison of algorithms, as continuous control tasks often require additional considerations and modifications. Finally, the \num{100000} interactions limit strikes a balance between providing enough data for meaningful evaluation and being computationally feasible within our resource constraints, especially considering the large number of experiments required for each algorithm, as detailed in section~\vref{subsubsec:number_runs}.

These factors combined make Atari 100k a practical and effective choice for our benchmark, enabling us to achieve our project goals efficiently.


\subsection{Experiment Setup}
\label{subsec:experiment_setup}
In this section we will address all the decisions made in the setup of the experiments.

\subsubsection{Number of Runs}
\label{subsubsec:number_runs}

In determining how many runs to carry out during the experimentation and testing of a reinforcement learning algorithm, at least two fundamental aspects must be taken into account: the high variance of reinforcement learning, and thus its high susceptibility to randomness, and the evaluation of the generality of the algorithm, which must therefore be tested in several different environments in order to actually prove that it is capable of solving multiple problems and not just be ultra-specialized on a single use-case.

In addressing the first aspect we can refer to the literature to get an idea of how many runs with different seeds are usually performed to alleviate this problem. If in the early days of RL (and not DRL) the number of runs stood at around 100 and in any case did not fall below 30, at least until the introduction of ALE (Arcade Learning Environment)~\cite{bellemare:ale} included, with the advent of DRL the number of runs was consistently reduced to 5 or less because of the high cost in terms of time and resources per run. Although this has been the standard for years, a more recent work~\cite{agarwal:statistical_precipice} has shown that this is the source of a problem. Practitioners use point estimates such as mean and median to aggregate performances  scores across tasks to summarize the results of the various runs, but this metrics are not the best way to do so because they ignore the statistical uncertainty inherent in performing only a few runs.

In particular, the study points out that in the case of Atari at least 100 runs per environment are required to obtain robust results, a value that is, however, impractical in reality. To address this, the study recommends using alternative aggregation metrics, such as interquartile means, designed precisely to obtain more efficient and robust estimates and have small uncertainty even with a handful of runs, since they are not overly affected by outliers like the point estimates.

In our case we will be forced to limit ourselves to 4 runs per environment, so we will use, in addition to the more classic and popular metrics such as the point estimates mentioned above, the other metrics suggested in~\cite{agarwal:statistical_precipice}.  It should anyway be noted that a low number of runs is a less significant problem for us, since we are not attempting to advance the state of the art performance of DRL algorithms, but have instead a focus on energy consumption and emissions, which should in any case remain constant regardless of the actual learning of the agent, which is instead related to randomness.

With regard to the second aspect, namely, testing the algorithms on a variety of environments to evaluate their generality, Atari 100k once again comes to our aid, being constituted by 26 games. Moreover, the Arcade Learning Environment, built on top of the Atari 2600 emulator Stella and used by gymnasium, includes over 55 games. Unfortunately, again, we do not have the time and/or computational resources to test on all the Atari \num{100}k's 26 games or all the ones available in ALE, so we selected for the benchmark a representative subset of 8 Atari games, trying to choose games that cover a range of difficulties and styles. Obviously, with so few games because of the constraints just mentioned, an exhaustive selection is difficult, but we nonetheless tried to provide a balanced benchmark, ensuring that the selected games cover a range of challenges to effectively evaluate different algorithms, while still not being excessively difficult. This last requirement is due to basic DQN and its more simple extensions, which have some limitations in only 100k interactions (the team that introduced the DQNs trained its model on 2 million interactions to achieve interesting results).

Here are the 8 selected games, completed with a rationale for their inclusion:
\begin{itemize}
	\item \textit{Alien} - moderate difficulty, good for testing exploration and strategy;
	\item \textit{Amidar} - requires planning and quick decision-making;
	\item \textit{Assault} - tests reflexes and targeting accuracy;
	\item \textit{Boxing} - simple but requires precise control and timing;
	\item \textit{Breakout} - classic game, good for testing control;
	\item \textit{Freeway} - simple yet tests quick decision-making under pressure;
	\item \textit{Ms. Pac-Man} - classic maze game, tests navigation and evasion;
	\item \textit{Pong} - simple and well-understood, great for baseline comparisons.
\end{itemize}
Although Alien and Ms. Pac-Man may appear similar in terms of overall theme, we decided to keep both in our selection due to their differing action space structures. Alien has a more complex movement and shooting action space, while Ms. Pac-Man involves navigation-based control with a different interaction model. Including both allows us to evaluate how reinforcement learning algorithms adapt to environments with distinct control dynamics, rather than just variations in visual style or game mechanics.

So, to summarize, each algorithm will be evaluated on 8 different Atari games, with 4 runs per game using different random seeds, for a total of 32 trainings per algorithm. This approach, with appropriate metrics, ensures that our results are statistically significant and account for the inherent variability in RL training processes.

\subsubsection{Data Logging and Storage}
\label{subsubsec:data_collection}

Collecting comprehensive and accurate data is crucial for evaluating both the performance and energy consumption of the algorithms. We employ several tools and services to ensure robust data collection and analysis.

To track the performance metrics, we use both online and local tools. The online service \textit{Weights and Biases} is used for real-time monitoring and storage of experimental data. This platform allows for easy sharing and collaboration, as well as providing powerful visualization and analysis tools. Locally, we use \textit{TensorBoard}, which integrates seamlessly with our training workflows and offers detailed insights into the training process through its rich set of visualizations.

In addition to tracking performance metrics, monitoring energy consumption is the key aspect of the project. For this we use \textit{CodeCarbon}~\cite{benoit:code_carbon}, a tool designed to measure the carbon footprint of computing activities. CodeCarbon is integrated in our training scripts to provide real-time tracking of energy usage, a crucial metric for comparing the energy efficiency of the different algorithms.

The metrics we collect include:
\begin{itemize}
	\item \textit{Global Step:} indicates the number of environment interactions during training.
	\item \textit{Episodic Return:} the score achieved in each episode, providing a measure of the algorithm's performance.
	\item \textit{Loss:} tracks the optimization process, giving insight into the learning dynamics of the algorithm.
	\item \textit{Value Estimates:} such as Q-values or value function estimates, offering insight into the agent's decision-making process.
	\item \textit{Policy Entropy:} measures the randomness in the policy and how much it differs from the previous one, useful for understanding exploration behavior and how much room for improvement is still left.
	\item \textit{Learning Rate:} the rate at which the model learns, especially if it changes during training.
	\item \textit{Emissions:} the amount of CO2 emitted during training, tracked by CodeCarbon, allowing us to evaluate the energy efficiency of each algorithm.
\end{itemize}

Weights and Biases facilitates a coarse aggregation and visualization of these metrics across multiple runs and environments, making it easier to compare results at a first glance and draw some first insights. TensorBoard provide supplementary local visualizations to help diagnose any issues during training and ensure the integrity of the collected data.

By using these tools in tandem, we aim to collect a comprehensive dataset that covers both the performance and energy consumption aspects of the algorithms, ensuring a thorough evaluation aligned with the goals of our project.

\subsubsection{Development and Execution Environment}
\label{subsubsec:development_execution_environment}

The development and execution environment for the project involves both hardware and software. In particular, we have made use of two different hardware setups due to constraints in energy tracking capabilities.

Initially, all configurations and the first fine-tuning of DQN and some other algorithms were performed on a machine with:
\begin{itemize}
	\item \textbf{CPU}: 11th Gen Intel(R) Core(TM) i5-11400F @ 2.60GHz
	\item \textbf{GPU}: NVIDIA GeForce GTX 1050 Ti
	\item \textbf{RAM}: 16GB
\end{itemize}

However, due to CodeCarbon's lack of support for the GTX 1050 Ti in tracking GPU energy consumption, the main training experiments had to be conducted on a different machine with higher computational power and proper energy tracking support. The second setup consisted of:
\begin{itemize}
	\item \textbf{CPU}: Intel(R) Core(TM) i9-10980XE @ 3.00GHz
	\item \textbf{GPU}: NVIDIA RTX A5000
	\item \textbf{RAM}: 64GB
\end{itemize}

This switch was necessary to ensure accurate measurement of energy consumption and carbon emissions for reinforcement learning training. While the first machine was sufficient for setting up the environment and running initial fine-tuning, the A5000 GPU in the second setup provided both better computational efficiency and full compatibility with CodeCarbon, ensuring reliable energy tracking during experimentation.

On the software side, after careful considerations and some testing with other alternatives like OpenAI's \textit{Spinning Up}, we chose to base the implementation of the project on \textit{CleanRL}~\cite{huang:cleanrl}. As the authors states, CleanRL is an open-source library that provides high-quality single-file implementations of Deep Reinforcement Learning algorithms. It provides an environment already complete with most dependencies a project like ours might need (like Gymnasium), has a straightforward codebase, and already integrates tools like Weights and Biases and TensorBoard, that help log metrics, hyperparameters, videos of an agent's gameplay, dependencies, and more.

The single-file implementation philosophy of CleanRL aims to make reinforcement learning research more accessible and reproducible and make the performance-relevant details easier to recognize. By consolidating every algorithm codebase into single files, it simplifies the understanding and modification of algorithms, which is particularly beneficial for both educational purposes and rapid prototyping, even though it comes at the cost of losing modularity and duplicating some code.

We leverage CleanRL's existing implementations where available, tweaking them to meet the specific requirements of our benchmarks. When an implementation for a particular algorithm is not available, we develop it from scratch, trying to adhere to CleanRL's philosophy and implementation principles. This approach ensures consistency and comparability across all tested algorithms.

In the end, the environment for our experiments should be efficient and easily reproducible, facilitating the accurate evaluation of both performance and energy consumption of various deep reinforcement learning algorithms.


\subsubsection{Atari Environment Configuration}

The Atari environment setup follows best practices outlined in \cite{machado:revisiting_ale} for training and evaluating agents in the Arcade Learning Environment (ALE). These decisions were made to ensure a standardized, reproducible, and robust experimental setting. Additionally, we incorporate relevant insights from the ALE documentation to refine our environment configuration. Many of these choices also align with those made in the first works on Deep Q-Networks (DQN), ensuring comparability with early research efforts.

\paragraph{Preprocessing and Standardization}
The preprocessing pipeline ensures consistent input representations across different Atari games, avoiding confounding factors that could skew results. The following steps are implemented:

\begin{itemize}
	\item \textbf{Frame Skipping:} We use \texttt{MaxAndSkipEnv(skip=4)}, ensuring that actions are repeated for four frames and the maximum pixel values of consecutive frames are used. This stabilizes the input representation and allows agents to process meaningful changes in the game environment while reducing computational load \cite{machado:revisiting_ale}.
	\item \textbf{Random No-op Initialization:} At the beginning of each episode, a random number (up to 30) of "do nothing" actions are executed (\texttt{NoopResetEnv(noop\_max=30)}). This prevents deterministic policies from exploiting fixed starting conditions, improving generalization \cite{machado:revisiting_ale}.
	\item \textbf{Episodic Life and Fire Reset:} 
	\begin{itemize}
		\item \texttt{EpisodicLifeEnv} is used to reset the environment after each lost life instead of at the end of the full game. This makes training more efficient by exposing the agent to more starting states per episode.
		\item \texttt{FireResetEnv} is applied in games where a "FIRE" action is required to start (e.g., Breakout), ensuring proper initialization.
	\end{itemize}
	\item \textbf{Observation Preprocessing:}
	\begin{itemize}
		\item Raw RGB images are converted to grayscale (\texttt{GrayScaleObservation}) and resized to 84×84 pixels (\texttt{ResizeObservation}).
		\item A history of the last four frames is stacked (\texttt{FrameStack(4)}) to provide temporal context, compensating for the lack of explicit memory.
	\end{itemize}
	\item \textbf{Reward Clipping:} Rewards are clipped between -1 and 1 (\texttt{ClipRewardEnv}) to standardize their scale across different games. This makes the algorithms able to work with all games without needing refinements to adapt to particularly high- or low-reward games, while stabilizing training.
\end{itemize}

\paragraph{Choice of NoFrameskip-v4 Environments}
In this study, we opted to use the \textbf{NoFrameskip-v4} versions of the Atari environments~\cite{farama:ale}. The rationale behind this choice includes:

\begin{itemize}
	\item \textbf{Deterministic Execution for Reproducibility:} The \texttt{v4} version does not introduce stochastic frame skipping, ensuring that each run is fully deterministic given a fixed random seed. This is crucial for benchmarking energy consumption and algorithmic efficiency without introducing additional variance from the environment itself.
	\item \textbf{Explicit Frame Skipping Control:} Rather than relying on ALE's internal frame skipping, we apply \texttt{MaxAndSkipEnv(skip=4)}, allowing direct control over action repeat frequency. This ensures consistency across all environments and aligns with best practices recommended in \cite{machado:revisiting_ale}.
	\item \textbf{Alignment with Prior Research:} Many deep reinforcement learning studies, including those evaluating sample efficiency in Atari benchmarks, use NoFrameskip-v4 environments (or equivalent at the time) to ensure fair comparisons and avoid unintended interactions between stochastic skips and algorithm behavior. Our setup is almost identical to the v5 version of the environments, which includes all the best practices in~\cite{machado:revisiting_ale}, except that we do not use sticky actions (\texttt{repeat\_action\_probability=0.25}).
	\item \textbf{Impact of Sticky Actions in Short-Horizon Training:} Sticky actions introduce a 25\% probability of repeating the last action, adding stochasticity to the environment. While this can improve generalization in long training runs (millions of steps), in our case—where only 100k iterations are allowed—removing sticky actions is not only not problematic, but almost mandatory. The added stochasticity would significantly degrade performance in such a short training horizon, leading to instability and inefficient learning. Our approach prioritizes stability and faster convergence, which is even more crucial in low-resource settings. Moreover, their usage is not ubiquitous in the literature as the other options we discussed.
\end{itemize}

\paragraph{Conclusion}
By structuring our preprocessing pipeline around the NoFrameskip-v4 environments and following the best practices from \cite{machado:revisiting_ale}, we ensure that our experimental results are robust, reproducible, and comparable to prior research. The preprocessing steps applied in our implementation are widely used in reinforcement learning studies and enable fair performance evaluations across different Atari games. Furthermore, the decision to exclude sticky actions aligns with the constraints of our 100k iteration limit, ensuring meaningful training without excessive randomness hindering learning progress.

\begin{table}[h]
	\centering
	\begin{tabular}{|l|c|c|}
		\hline
		\textbf{Feature} & \textbf{NoFrameskip-v4 (Our Setup)} & \textbf{ALE v5} \\
		\hline
		\textbf{Frame Skipping} & Explicitly set via \texttt{MaxAndSkipEnv(skip=4)} & Implicit (default 4) \\
		\hline
		\textbf{No-op Start} & \texttt{NoopResetEnv(noop\_max=30)} & \texttt{NoopResetEnv(noop\_max=30)} \\
		\hline
		\textbf{Episodic Life} & \texttt{EpisodicLifeEnv} & \texttt{EpisodicLifeEnv} \\
		\hline
		\textbf{Fire Reset} & \texttt{FireResetEnv} (if needed) & \texttt{FireResetEnv} (if needed) \\
		\hline
		\textbf{Observation Preprocessing} & 
		Grayscale + Resize (84x84) + FrameStack(4) & 
		Grayscale + Resize (84x84) + FrameStack(4) \\
		\hline
		\textbf{Reward Clipping} & \texttt{ClipRewardEnv} (-1, 1) & \texttt{ClipRewardEnv} (-1, 1) \\
		\hline
		\textbf{Sticky Actions (\texttt{repeat\_action\_probability})} & Not Used (Fixed Action Selection) & Enabled (\texttt{0.25}) \\
		\hline
	\end{tabular}
	\caption{Comparison between our setup based on NoFrameskip-v4 and ALE v5 environments.}
	\label{tab:ale_comparison}
\end{table}


\subsection{Data Analysis and Visualization}
\label{subsec:data_analysis}

A critical part of this project involved consolidating and analyzing the training logs in a consistent and reproducible manner. Although \textit{Weights \& Biases} (W\&B) and \textit{TensorBoard} can both display metrics across runs, they each have limitations for comparative analysis—particularly when plotting multiple algorithms or combining results with additional metadata (e.g., hyperparameters, emissions data). Consequently, a custom data-processing pipeline was built to generate unified plots and aggregated statistics.

\subsubsection{Log Collection and Merging}
We collected detailed logs for each run: TensorBoard event files (containing metrics such as episodic returns, steps per second, losses, etc.) and W\&B logs, which contains all the data of the TensorBoard logs (extrapolated from the uploaded TensorBoard logs), plus some other system related metrics. To work on this data in Python with its scientific tools we employed the \texttt{tbparse} library to parse the TensorBoard logs, making modifications to the library where necessary to handle deprecated NumPy types. These parsed logs resulted in two CSV files, one with all the metrics for all the runs, the other containing additional information about hyperparameters (e.g., learning rate, buffer size, and so forth) from the experimental configuration. We then merged the two files into a single, larger CSV dataset containing all runs from all algorithms.

\subsubsection{Normalization of Returns}
Since the raw episodic returns for Atari games vary widely in scale, a fair and not skewd comparison needed a normalization step. This is one of the reasons that prevented us from directly using tensorboard and wandb plots. We performed two distinct normalization procedures:
\begin{itemize}
	\item \textbf{Human-Normalized Returns.} In this approach, for each game, we subtract the score of a random agent and divide by the difference between the human baseline and the random baseline. This is a standard practice in Atari benchmarks to contextualize performance relative to human play.
	\item \textbf{Min-Max Normalization.} A classic min-max scaling (\emph{i.e.}, $x_{\text{norm}} = \frac{x - x_{\min}}{x_{\max} - x_{\min}}$) on a per-game basis, where $x_{\min}$ and $x_{\max}$ come from the observed range of returns for that game.
\end{itemize}
These normalized returns facilitate more intuitive cross-game comparisons, ensuring that no single game with unusually high or low rewards dominates the overall analysis. Since we don't care about comparing the performance of our implementations with the literature, and the relative comparison between the algorithms is the same with both normalizations, we can use indifferently either one of them based on which one produce a clearer plot.

\subsubsection{Interpolation and Aggregation}
When generating metric curves (such as episodic returns vs.\ training steps), we needed a consistent $x$-axis across runs. Many runs log metrics at slightly different steps (due to stochastic episode lengths, logging frequencies, etc.). The management of this aspect from both wandb and tensorboard is not ideal or lacking, not always allowing precise control or easy export of aggregated data.

In our pipeline, we therefore:
\begin{enumerate}
	\item \textbf{Filtered by Metric and Algorithm.} We grouped rows in the CSV by a specific tag (e.g., \texttt{charts/episodic\_return}, \texttt{charts/SPS}) and by algorithm.
	\item \textbf{Interpolated to a Common Grid.} For each subset, we created a uniformly spaced array of steps (e.g., \num{1000} points from \num{0} to \num{100000}. We then applied linear interpolation on each run's time series to ensure all runs aligned on this common step axis.
	\item \textbf{Computed Statistics.} At each point on the new, shared step grid, we aggregated the interpolated run values to produce statistics such as \textit{mean}, \textit{min--max range}, \textit{standard deviation}, etc.
\end{enumerate}
The interpolation ensures that every run contributes to the curves at the same discrete set of training steps, simplifying the generation of \emph{mean} or \emph{min--max} envelopes. This was crucial for plotting aggregate performance over multiple runs.

\subsubsection{Plot Generation and CSV Output}
Following the interpolation and aggregation process, the final step was to produce consistent plots for each metric--algorithm pair. We used \texttt{matplotlib} to generate both raster (PNG) and vector (SVG) graphics, giving us flexibility for both online presentation and printed material. Additionally, the aggregated statistics for each plot were saved as a separate CSV file, allowing subsequent combinations of multiple algorithms on a single plot without re-running the entire pipeline.

Overall, this approach provided:
\begin{itemize}
	\item Fine-grained control over which metrics and runs to include;
	\item A robust method (interpolation) to align metrics across stochastic training steps;
	\item Easy export to consistent plots and CSVs for further analysis.
\end{itemize}
By integrating custom plotting and data analysis with the logs from W\&B and TensorBoard, we ensure reproducibility and enable deeper insights into the trade-offs between performance and energy consumption across all tested algorithms.

