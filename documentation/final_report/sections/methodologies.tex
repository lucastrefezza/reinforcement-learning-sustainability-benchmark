\section[Methodological Steps]{Methodological Steps}
\label{sec:methodologies}

The methodology used follows from the basic idea of this benchmark: to execute all the algorithms for the same number of environment interactions, enabling a direct comparison of both their achieved scores and energy consumption.
An alternative comparison would involve taking the score of the lowest-performing algorithm from the initial trial and retraining all algorithms until they achieve that score. This would allow us to compare the time and energy required by each algorithm to reach a common performance level. Unfortunately, time and resources constraints make retraining all algorithms unfeasible, therefore, we will either approximate this comparison by using the logged training returns from the initial trial, or delay this comparison to future works. The logging includes the \verb*|global_step|, indicating the environment interaction we are at, and the \verb*|episodic_return|, which is the return of the episode (i.e., the score on which to compare), as well as all performance and power consumption data up to that point. By analyzing these logs, we can estimate how much time and energy each algorithm would take to reach the score obtained by the lowest performer in the initial trial. 

The following sections outline the several key steps involved in the methodology adopted for this project.

\subsection{Algorithms Selection}
\label{subsec:algorithm_selection}

As stated, in our benchmark we consider both value-based methods and policy gradient methods. The selected algorithms are chosen to represent a wide range of approaches within both categories.

\subsubsection{Value-Based Methods}

Value-based methods are algorithms based on the approximation of a value function. The following algorithms were considered in this category (but due to time and computational constraints, only a subset of 5 of them were fully trained and evaluated):

\begin{itemize}
	\item \textit{Deep Q-Network (DQN)}: the first example of success in deep reinforcement learning, will serve as a sort of baseline for our benchmark.
	\item \textit{RAINBOW}~\cite{hessel:rainbow}: an advanced method that combines several enhancements to the original DQN. Each improvement will also be tested individually to assess its contributions to energy consumption and performance. These are listed hereafter:
	\begin{itemize}
		\item Double Q-Learning (Double DQN)~\cite{van:double_q};
		\item Prioritized Experience Replay~\cite{schaul:prioritized};
		\item Dueling Network Architectures~\cite{wang:dueling};
		\item Multi-step / N-step Learning~\cite{peng:incremental};
		\item Distributional RL~\cite{bellemare:distributional};
		\item Noisy Nets~\cite{fortunato:noisy};
	\end{itemize}
	\item \textit{Self-Predictive Representations (SPR)}~\cite{schwarzer:spr}: a recent method that leverages self-predictive representations to enhance efficiency.
\end{itemize}

\subsubsection{Policy Gradient Methods}

Policy gradient methods directly approximate the policy and include as a special case the actor-critic methods, which simultaneously approximate a policy and a value function. The algorithms considered in this category are:
\begin{itemize}
	\item \textit{REINFORCE}~\cite[Chapter~13]{sutton:rl}: a basic policy gradient method, or its variant REINFORCE with baseline (also known as Vanilla Policy Gradient, VPG).
	\item \textit{Proximal Policy Optimization (PPO)}~\cite{schulman:ppo}: a popular and efficient policy gradient method that uses a clipped objective to improve training stability.
	\item \textit{Deep Deterministic Policy Gradient (DDPG)}~\cite{lillicrap:ddpg}: an algorithm that combines policy gradients with deterministic policy updates for continuous action spaces.
	\item \textit{Twin Delayed DDPG (TD3)~}\cite{fujimoto:td3}: an improvement over DDPG that addresses function approximation errors through various techniques, such as delayed policy updates and target policy smoothing.
	\item \textit{Soft Actor-Critic (SAC)}~\cite{haarnoja:sac}: an extension of DDPG that incorporates entropy regularization to encourage exploration. SAC, like TD3, uses two Q-networks to reduce overestimation bias, but it differs by optimizing a stochastic policy instead of a deterministic one. This makes SAC more sample-efficient and stable in continuous control tasks. It is also more easily adapted to discrete action spaces.
	\item \textit{Data-Regularized Q (DRQ)}~\cite{kostrikov:drq}: a method that incorporates data augmentation to regularize the training of Q functions, improving performance and stability.
\end{itemize}


\subsection{Task Selection}
\label{subsec:task_selection}

Regarding the task on which to compare the algorithms, there were several suitable candidates: Atari 100k~\cite{kaiser:atari100k}, a continuous control task from the DeepMind Control Suite, or one of the many other task (besides Atari) included in OpenAI Gymnasium (formerly Gym). After various tests and research we opted for the Atari 100k benchmark, a discrete task that consists of playing selected Atari games for only \num{100000} environment interactions.

The reason for this choice is multifaceted. Atari 100k is a widely used benchmark in the DRL community, the wealth of available prior research and baseline results facilitates a more straightforward validation and comparison of our experimental results with those from other studies and algorithms. It is also well suited for evaluating the performance of almost all popular DRL algorithms, ensuring a comprehensive assessment. Additionally, Atari games provide a range of different challenges, including planning, reaction time, and strategy, making it a robust benchmark for assessing general DRL capabilities.

Moreover, the discrete nature of Atari 100k simplifies the implementation and comparison of algorithms, as continuous control tasks often require additional considerations and modifications. Finally, the \num{100000} interactions limit strikes a balance between providing sufficient data for meaningful evaluation and remaining computationally feasible within our resource constraints, especially considering the large number of experiments required for each algorithm, as detailed in section~\vref{subsubsec:number_runs}.

These factors combined make Atari 100k a practical and effective choice for our benchmark, enabling us to achieve our project goals efficiently.


\subsection{Experiment Setup}
\label{subsec:experiment_setup}
In this section we will address all the decisions made in the setup of the experiments.

\subsubsection{Number of Runs}
\label{subsubsec:number_runs}

In determining how many runs to carry out during the experimentation and testing of a reinforcement learning algorithm, at least two fundamental aspects must be taken into account: the high variance of reinforcement learning, and thus its high susceptibility to randomness, and the evaluation of the generality of the algorithm, which must therefore be tested in several different environments in order to actually prove that it is capable of solving multiple problems and not just be ultra-specialized on a single use-case.

In addressing the first aspect we can refer to the literature to get an idea of how many runs with different seeds are usually performed to alleviate this problem. If in the early days of RL (and not DRL) the number of runs stood at around 100 and in any case did not fall below 30, at least until the introduction of ALE (Arcade Learning Environment)~\cite{bellemare:ale} included, with the advent of DRL the number of runs was consistently reduced to 5 or less because of the high cost in terms of time and resources per run. Although this has been the standard for years, a more recent work~\cite{agarwal:statistical_precipice} has shown that this is the source of a problem. Practitioners use point estimates such as mean and median to aggregate performances  scores across tasks to summarize the results of the various runs, but this metrics are not the best way to do so because they ignore the statistical uncertainty inherent in performing only a few runs.

In particular, the study points out that in the case of Atari at least 100 runs per environment are required to obtain robust results, a value that is, however, impractical in reality. To address this, the study recommends using alternative aggregation metrics, such as interquartile means, designed precisely to obtain more efficient and robust estimates and have small uncertainty even with a handful of runs, since they are not overly affected by outliers like the point estimates.

In our case we will be forced to limit ourselves to 4 runs per environment, so we will use, in addition to the more classic and popular metrics such as the point estimates mentioned above, the other metrics suggested in~\cite{agarwal:statistical_precipice}.  It should anyway be noted that a low number of runs is a less significant problem for us, since we are not attempting to advance the state of the art performance of DRL algorithms, but have instead a focus on energy consumption and emissions, which should in any case remain constant regardless of the actual learning of the agent, which is instead related to randomness.

With regard to the second aspect, namely, testing the algorithms on a variety of environments to evaluate their generality, Atari 100k once again comes to our aid, being constituted by 26 games. Moreover, the Arcade Learning Environment, built on top of the Atari 2600 emulator Stella and used by gymnasium, includes over 55 games. Unfortunately, again, we do not have the time and/or computational resources to test on all the Atari \num{100}k's 26 games or all the ones available in ALE, so we selected for the benchmark a representative subset of 8 Atari games, trying to choose games that cover a range of difficulties and styles. Obviously, with so few games because of the constraints just mentioned, an exhaustive selection is difficult, but we nonetheless tried to provide a balanced benchmark, ensuring that the selected games cover a range of challenges to effectively evaluate different algorithms, while still not being excessively difficult. This last requirement is due to basic DQN and its more simple extensions, which have some limitations in only 100k interactions (the team that introduced the DQNs trained its model on millions of interactions to achieve interesting results).

Here are the 8 selected games, along with a rationale for their inclusion:
\begin{itemize}
	\item \textit{Alien} - involves exploration and strategic movement;
	\item \textit{Amidar} - requires precise movement, quick decision-making and long-term planning;
	\item \textit{Assault} - a fast-paced shooter testing reflexes and targeting accuracy;
	\item \textit{Boxing} - visually simple yet requires precise timing and positioning;
	\item \textit{Breakout} - a control-based game widely studied in RL;
	\item \textit{Freeway} - simple ruleset, tests quick decision-making and reaction time;
	\item \textit{Ms. Pac-Man} - emphasizes navigation, evasion, and planning;
	\item \textit{Pong} - minimalistic, simple and well-understood game used as an RL baseline.
\end{itemize}

Although Alien and Ms. Pac-Man may appear similar in terms of overall theme, we decided to keep both in our selection due to their differing action space structures. Alien has a more complex movement and shooting action space, while Ms. Pac-Man involves navigation-based control with a different interaction model. Including both allows us to evaluate how reinforcement learning algorithms adapt to environments with distinct control dynamics, rather than just variations in visual style or game mechanics.

So, to summarize, each algorithm will be evaluated on 8 different Atari games, with 4 runs per game using different random seeds, for a total of 32 trainings per algorithm. This approach, with appropriate metrics, ensures that our results are statistically significant and account for the inherent variability in RL training processes.

\subsubsection{Data Logging and Storage}
\label{subsubsec:data_collection}

Collecting comprehensive and accurate data is crucial for evaluating both the performance and energy consumption of the algorithms. We employ several tools and services to ensure robust data collection and analysis.

To track the performance metrics, we use both online and local tools. The online service \textit{Weights and Biases} (wandb) is used for real-time monitoring and storage of experimental data. This platform allows for easy sharing and collaboration, as well as providing powerful visualization and analysis tools. Locally, we use \textit{TensorBoard}, which integrates seamlessly with our training workflows and offers detailed insights into the training process through its rich set of visualizations.

In addition to tracking performance metrics, monitoring energy consumption and emissions is the key aspect of the project. For this we use \textit{CodeCarbon}, a tool designed to measure the carbon footprint of computing activities. As stated in their documentation, this package enables developers to track emissions, measured as kilograms of \mbox{$\text{CO}_2$-equivalents} ($\text{CO}_2$eq) in order to estimate the carbon footprint of their work. \mbox{$CO_2$-eq} is a standardized measure used to express the global warming potential of various greenhouse gases: the amount of $\text{CO}_2$ that would have the equivalent global warming impact. For computing, which emits $\text{CO}_2$ via the electricity it is consuming, carbon emissions are measured in kilograms of $\text{CO}_2$-equivalent per kilowatt-hour~\cite{benoit:code_carbon}. See \href{https://mlco2.github.io/codecarbon/methodology.html}{this} page and section \ref{subsec:exp_setup_adjustments} for more information on their methodology.

Explained the tools, the metrics we collect through them include:
\begin{itemize}
	\item \textit{Global Step:} indicates the number of environment interactions during training.
	\item \textit{Episodic Return:} the score achieved in each episode, providing a measure of the algorithm's performance.
	\item \textit{Loss(es):} track the optimization process, giving insight into the learning dynamics of the algorithm.
	\item \textit{Value Estimates:} such as Q-values or value function estimates, offering insight into the agent's decision-making process.
	\item \textit{Policy Entropy:} measures the randomness in the policy and how much it differs from the previous one, useful for understanding exploration behavior and how much room for improvement is still left.
	\item \textit{Learning Rate:} the rate at which the model learns, especially if it changes during training.
	\item \textit{Emissions:} the amount of $CO_2$-eq emitted during training, tracked by CodeCarbon.
\end{itemize}

Weights and Biases facilitates a coarse aggregation and visualization of these metrics across multiple runs and environments, making it easier to compare results at a first glance and draw some first insights. TensorBoard provide supplementary local visualizations to help diagnose any issues during training and ensure the integrity of the collected data.

By using these tools in tandem, we aim to collect a comprehensive dataset that covers both the performance and energy consumption aspects of the algorithms, ensuring a thorough evaluation aligned with the goals of our project.

\subsubsection{Development and Execution Environment}
\label{subsubsec:development_execution_environment}

The development and execution environment for the project involves both hardware and software. In particular, we have made use of two different hardware setups due to constraints in energy tracking capabilities.

Initially, all configurations and the first fine-tuning of DQN and some other algorithms were performed on a machine with:
\begin{itemize}
	\item \textbf{CPU}: 11th Gen Intel(R) Core(TM) i5-11400F @ 2.60GHz
	\item \textbf{GPU}: NVIDIA GeForce GTX 1050 Ti
	\item \textbf{RAM}: 16GB
\end{itemize}

However, due to CodeCarbon's lack of support for the GTX 1050 Ti in tracking GPU energy consumption, the main training experiments had to be conducted on a different machine with higher computational power and proper energy tracking support. The second setup consisted of:
\begin{itemize}
	\item \textbf{CPU}: Intel(R) Core(TM) i9-10980XE @ 3.00GHz
	\item \textbf{GPU}: NVIDIA RTX A5000
	\item \textbf{RAM}: 64GB
\end{itemize}
While the first machine was sufficient for setting up the environment and running initial fine-tuning, this switch to the A5000 GPU in the second setup was necessary to ensure full compatibility with CodeCarbon and reliable, accurate measurement of energy consumption and carbon emissions during experimentation.

On the software side, after careful consideration and some testing with other alternatives like OpenAI's \textit{Spinning Up}, we chose to base the implementation of the project on \textit{CleanRL}~\cite{huang:cleanrl}. As the authors state, CleanRL is an open-source library that provides high-quality single-file implementations of Deep Reinforcement Learning algorithms. It provides an environment already complete with most dependencies a project like ours might need (like Gymnasium), has a straightforward codebase, and already integrates tools like Weights and Biases and TensorBoard, that help log metrics, hyperparameters, videos of an agent's gameplay, dependencies, and more.

The single-file implementation philosophy of CleanRL aims to make reinforcement learning research more accessible and reproducible and make the performance-relevant details easier to recognize. By consolidating every algorithm codebase into single files, it simplifies the understanding and modification of algorithms, which is particularly beneficial for both educational purposes and rapid prototyping, even though it comes at the cost of losing modularity and duplicating some code.

We leverage CleanRL's existing implementations where available, tweaking them to meet the specific requirements of our benchmarks. When an implementation for a particular algorithm is not available, we develop it from scratch, trying to adhere to CleanRL's philosophy and implementation principles. This approach ensures consistency and comparability across all tested algorithms.

In the end, the environment for our experiments should be efficient and easily reproducible, facilitating the accurate evaluation of both performance and energy consumption of various deep reinforcement learning algorithms.


\subsubsection{Atari Environment Configuration}

The Atari environment setup follows best practices outlined in \cite{machado:revisiting_ale} for training and evaluating agents in the Arcade Learning Environment (ALE). These decisions were made to ensure a standardized, reproducible, and robust experimental setting. Additionally, we incorporate relevant insights from the ALE documentation to refine our environment configuration. Many of these choices also align with those made in the first works on Deep Q-Networks (DQN), ensuring comparability with early research efforts.

\paragraph{Preprocessing and Standardization}
The preprocessing pipeline ensures consistent input representations across different Atari games, avoiding confounding factors that could skew results. Through the use of appropriate atari wrappers of Stable Baselines~v3, the following steps are implemented:
\begin{description}
	\item[Frame skipping:] we use \verb|MaxAndSkipEnv(skip=4)|, ensuring that actions are repeated for four frames and the maximum pixel values of consecutive frames are used. This stabilizes the input representation and allows agents to process meaningful changes in the game environment while reducing computational load (the agent can  play roughly 4 times more games without significantly increasing the runtime).
	\item[Random no-op initialization:] at the beginning of each episode, a random number (up to 30) of "do nothing" actions are executed (\verb|NoopResetEnv(noop_max=30)|). This prevents deterministic policies from exploiting fixed starting conditions, improving generalization.
	\item[Episodic life and fire reset:]\
	\begin{description}
		\item[\texttt{EpisodicLifeEnv}:] is used to reset the environment after each lost life instead of at the end of the full game. This makes training more efficient by exposing the agent to more starting states per episode.
		\item[\texttt{FireResetEnv}:] is applied in games where a "FIRE" action is required to start (e.g., Breakout), ensuring proper initialization.
	\end{description}
	\item[Observation preprocessing:]\
	\begin{itemize}
		\item raw RGB images are converted to grayscale and resized to 84×84 pixels (\verb|GrayScaleObservation| and \verb|ResizeObservation|).
		\item a history of the last four frames is stacked (\verb|FrameStack(4)|) to provide temporal context, compensating for the partially observable nature of the environment.
	\end{itemize}
	\item[Reward clipping:] rewards are clipped between -1 and 1 (\verb|ClipRewardEnv|) to standardize their scale across different games. This makes the algorithms able to work with all games without needing refinements to adapt to particularly high- or low-reward games, while stabilizing training.
\end{description}

\paragraph{Choice of the Environments Version}
The Arcade Learning Environment (ALE)~\cite{bellemare:ale} provides multiple versions of Atari environments~\cite{farama:ale} to address different research requirements and needs. These environments versions encapsulate the preprocessing steps we talked about, each one setting a different default value for them and some other aspects of the games. Two of the most widely used versions are \textit{NoFrameskip-v4} and \textit{v5}. The main distinction between these is the inclusion of \textit{sticky actions} in \textit{v5}, as recommended in~\cite{machado:revisiting_ale}. Sticky actions introduce a 25\% probability of repeating the previous action, adding stochasticity to the environment to prevent overfitting when training deterministic policies. 

In this study, we use the \textit{NoFrameskip-v4} environments~\cite{farama:ale}. This choice is motivated by several factors. First, \textit{NoFrameskip-v4} ensures fully deterministic execution when a fixed random seed is used, which is crucial for the reproducibility of our experiments. This determinism allows us to conduct controlled comparisons of different algorithms while minimizing the influence of environment stochasticity on performance evaluation. Additionally, since our preprocessing pipeline explicitly applies \verb|MaxAndSkipEnv(skip=4)| to handle frame skipping in a standardized way (as discussed in the previous section), the built-in frame skipping behavior of other environment versions is unnecessary and would introduce redundant processing.

The primary difference between our setup and the \textit{v5} environments is the exclusion of sticky actions. While sticky actions can enhance generalization in long training regimes by preventing the agent from overfitting to deterministic game mechanics, their benefits are less relevant in our setting, where each run is limited to only 100k interactions. Under such a short training horizon, the additional stochasticity introduced by sticky actions would significantly degrade training stability and learning efficiency, leading to noisier performance estimates, thus removing them is not only not problematic, but almost mandatory. Furthermore, the use of sticky actions is not as ubiquitous in the reinforcement learning literature as other preprocessing steps, making their exclusion a reasonable choice also for comparability with prior work.

By structuring our preprocessing pipeline around the NoFrameskip-v4 environments and following the best practices from \cite{machado:revisiting_ale}, we ensure that our experimental results are robust, reproducible, and comparable to the large body of prior deep reinforcement learning research. The preprocessing steps applied in our implementation are widely used in reinforcement learning studies and enable fair performance evaluations across different Atari games. Furthermore, the decision to exclude sticky actions aligns with the constraints of our 100k iteration limit, ensuring meaningful training without excessive randomness hindering learning progress. Table~\ref{tab:ale_comparison} shows a comparison between our setup and v5.

\begin{table}
	\caption{Comparison between our setup based on NoFrameskip-v4 and ALE v5 environments.}
	\label{tab:ale_comparison}
	\centering
	\makebox[\textwidth]{%
	\begin{tabularx}{1.2\textwidth}{XXX}
		\toprule
		\textbf{Feature} & \textbf{NoFrameskip-v4 (Our Setup)} & \textbf{ALE v5} \\
		\midrule
		Frame Skipping & Explicitly set via \texttt{MaxAndSkipEnv(skip=4)} & Implicit (default 4) \\
		\midrule
		No-op Start & \texttt{NoopResetEnv(noop\_max=30)} & \texttt{NoopResetEnv(noop\_max=30)} \\
		\midrule
		Episodic Life & \texttt{EpisodicLifeEnv} & \texttt{EpisodicLifeEnv} \\
		\midrule
		Fire Reset & \texttt{FireResetEnv} (if needed) & \texttt{FireResetEnv} (if needed) \\
		\midrule
		Observation Preprocessing & 
		Grayscale + Resize (84x84) + FrameStack(4) & 
		Grayscale + Resize (84x84) + FrameStack(4) \\
		\midrule
		Reward Clipping & \texttt{ClipRewardEnv} (-1, 1) & \texttt{ClipRewardEnv} (-1, 1) \\
		\midrule
		Sticky Actions (\texttt{repeat\_action\_probability}) & Not Used (Fixed Action Selection) & Enabled (\texttt{0.25}) \\
		\bottomrule
	\end{tabularx}
	}%
\end{table}

\subsubsection{(Hyper)Parameter Configurations}

We discuss in this section a set of parameters that influence the experiment setup but not directly the optimization process like hyperparameters do. Regarding the latter, we do discuss here our general approach in their initial setting and optimization, but we delay to section \ref{sec:preliminary_results}, in which we dedicate a section to every algorithm, the details regarding their fine tuning, so to have a more cohesive and complete presentation. Both parameters and hyperparameters are passed to the script as command-line arguments.

\paragraph{Parameters}
\begin{description}
	\item[Tracking and Logging:] the flag \verb*|--track| ensures that training metrics are logged in \verb*|wandb|. The project name is set through the flag \verb*|--wandb-project-name| (in our case \texttt{rlsb}). The tracking in tensorboard is always enabled.
    \item[Device Usage:] \verb|--cuda| enables training on GPU, if this option is available.
	\item[Random Seed:] the \verb|--seed| value to use for this run.
	\item[Video Capture:] the \verb|--capture-video| flag is used to record the gameplay of the agent. We set it to \verb|False|, indicating that video recording of agent behavior is not performed during training, but we enable it during evaluation.
	\item[Model Saving:] the \verb|--save-model| flag is set to \verb|True|, this also automatically starts the evaluation process right after the model is saved.
\end{description}

\paragraph{Hyperparameters}
The hyperparameter selection for all implemented algorithms was primarily based on the configurations used in the original papers introducing each method and, when available, those from the CleanRL implementation. A key consideration across all algorithms was the adaptation of the hyperparameters strictly connected to the number of environment interactions. Since deep reinforcement learning algorithms are typically trained for 5 to 10 million interactions, whereas our study was constrained to \num{100000} interactions, certain hyperparameters, such as \texttt{learning\_starts} and \texttt{buffer\_size}, required adjustment to ensure appropriate behavior in this limited training setting.

For the baseline Deep Q-Network, the hyperparameters closely matched those from~\cite{mnih:human} and the CleanRL repository, as both sources used highly similar settings. The main focus of the tuning was the aforementioned adaptation to a reduced number of interactions with the environment.

For the various DQN variants tested, the hyperparameters were initialized using the same configuration employed for the base DQN implementation. The tuning process primarily involved modifying parameters directly associated with the respective architectural or algorithmic tweak while keeping the overall structure as consistent as possible with standard DQN. This approach aligns with the methodology commonly adopted in prior research introducing these modifications, ensuring a fair and controlled comparison. By maintaining a shared foundation across variants, we were able to isolate the impact of each specific enhancement in terms of performance improvements and emissions cost.

\subsubsection{Evaluation}

After completing the training phase, we evaluate the agent by executing \num{10} episodes in the target environment. During these episodes, we collect the \textit{episodic returns}, which serve as the primary metric for performance assessment.

The evaluation of the DQN-based methods is conducted with a fully deterministic policy, setting $\epsilon = 0.00$ to disable exploration, ensuring that the agent exploits its learned policy without stochasticity. This allows for a clear assessment of how well the trained model generalizes to unseen episodes. The same is true for SAC (see also the discussion in \ref{subsubsec:sac}), while Reinforce and PPO, being on-policy algorithm that optimize a stochastic policy, are evaluated on it, meaning actions are sampled from the learned distribution.

While, to avoid interference with the training process, we disable video recording during it, we enable it during evaluation by setting \texttt{capture\_video=True}. This provides visual insight into the agent's behavior without incurring the computational overhead during learning.

The collected episodic returns undergo statistical analysis following the methods described in \nameref{subsec:data_analysis}. Specifically:
\begin{itemize}
	\item We apply normalization to the collected data, both \textbf{human normalization} and \textbf{min-max normalization}, as detailed in \ref{subsubsec:normalization}.
	\item Basic statistics are computed on the normalized data, including mean, standard deviation, and median.
	\item The \textbf{interquartile mean (IQM)} is used as a robust estimator, as suggested in prior research.

\end{itemize}

The evaluation script loads the trained model, initializes a synchronized evaluation environment, and runs the agent for the specified number of episodes. It follows the same preprocessing pipeline used in training, ensuring consistency in observation space and action execution.

\subsection{Data Analysis and Visualization}
\label{subsec:data_analysis}

A critical part of this project involved consolidating and analyzing the training logs in a consistent and reproducible manner. Although \textit{Weights \& Biases} (W\&B) and \textit{TensorBoard} can both display metrics across runs, they each have limitations for comparative analysis—particularly when plotting multiple algorithms or combining results with additional metadata (e.g., hyperparameters, emissions data). Consequently, a custom data-processing pipeline was built to generate unified plots and aggregated statistics.

\subsubsection{Log Collection and Merging}
We collected detailed logs for each run: TensorBoard event files (containing metrics such as episodic returns, steps per second, losses, etc.) and W\&B logs, which contains all the data of the TensorBoard logs (extrapolated from the uploaded TensorBoard logs), plus some other system related metrics. To work on this data in Python with its scientific tools we employed the \texttt{tbparse} library to parse the TensorBoard logs, making modifications to the library where necessary to handle deprecated NumPy types. These parsed logs resulted in two CSV files, one with all the metrics for all the runs, the other containing additional information about hyperparameters (e.g., learning rate, buffer size, and so forth) from the experimental configuration. We then merged the two files into a single, larger CSV dataset containing all runs from all algorithms.

\subsubsection{Normalization of Returns}
\label{subsubsec:normalization}
Since the raw episodic returns for Atari games vary widely in scale, a fair and not skewd comparison needed a normalization step. This is one of the reasons that prevented us from directly using tensorboard and wandb plots. We performed two distinct normalization procedures:
\begin{description}
	\item[Human-Normalized Returns:] in this approach, for each game, we subtract the score of a random agent and divide by the difference between the human baseline and the random baseline. This is a standard practice in Atari benchmarks to contextualize performance relative to human play. Among the first to use this approach were the authors of~\cite{mnih:human}, from which we took the random policy and professional human player values used for normalization. The formula for obtaining the normalized value is: $x_{\text{norm}} = \frac{x - x_\text{random}}{x_\text{human} - x_\text{random}}$, where $x$ is the agent score, $x_\text{random}$ is the random policy score, and $x_\text{human}$ is the professional human player score.
	\item[Min-Max Normalization:] a classic min-max scaling (\emph{i.e.}, $x_{\text{norm}} = \frac{x - x_{\min}}{x_{\max} - x_{\min}}$) on a per-game basis, where $x_{\min}$ and $x_{\max}$ come from the observed range of returns for that game, over training and evaluation of all algorithms.
\end{description}
These normalized returns facilitate more intuitive cross-game comparisons, ensuring that no single game with unusually high or low rewards dominates the overall analysis. Since the relative comparison between the algorithms is the same with both normalizations, we can use indifferently either one of them based on which one produce a clearer plot.

\subsubsection{Interpolation and Aggregation}
When generating metric curves (such as episodic returns vs.\ training steps), we needed a consistent $x$-axis across runs. Many runs log metrics at slightly different steps (due to stochastic episode lengths, logging frequencies, etc.). The management of this aspect from both wandb and tensorboard is not ideal or lacking, not always allowing precise control or easy export of aggregated data.

In our pipeline, we therefore:
\begin{enumerate}
	\item \textbf{Filtered by Metric and Run.} We grouped rows in the CSV by a specific tag (e.g., \texttt{charts/episodic\_return}, \texttt{charts/SPS}) and by run.
	\item \textbf{Interpolated to a Common Grid.} For each subset, we created a uniformly spaced array of steps (i.e., \num{1000} points from \num{0} to \num{100000}). We then applied linear interpolation on each run's time series to ensure all runs aligned on this common step axis.
	\item \textbf{Computed Statistics.} At each point on the new, shared step grid, we aggregated the interpolated run values to produce statistics such as \textit{mean}, \textit{min--max range}, \textit{standard deviation}, \textit{iqmean} etc.
\end{enumerate}
The interpolation ensures that every run contributes to the curves at the same discrete set of training steps, simplifying the generation of \emph{mean} or \emph{min--max} envelopes. This was crucial for plotting aggregate performance over multiple runs.

\subsubsection{Plot Generation and CSV Output}
Following the interpolation and aggregation process, the final step was to produce consistent plots for each metric--algorithm pair. We used \texttt{matplotlib} to generate both raster (PNG) and vector (SVG) graphics. Additionally, the aggregated statistics for each plot were saved as a separate CSV file, allowing subsequent combinations of multiple algorithms on a single plot without re-running the entire pipeline.

Overall, this approach provided:
\begin{itemize}
	\item Fine-grained control over which metrics and runs to include;
	\item A robust method (interpolation) to align metrics across stochastic training steps;
	\item Easy export to consistent plots and CSVs for further analysis.
\end{itemize}
By integrating custom plotting and data analysis with the logs from W\&B and TensorBoard, we ensure reproducibility and enable deeper insights into the trade-offs between performance and energy consumption across all tested algorithms.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%The training procedures for reinforcement learning experiments require careful tuning of hyperparameters to ensure stability and performance across different environments and algorithms. In this section, we provide an overview of the hyperparameters used across all experimental runs and discuss general trends in their configurations. We also highlight parameters included in the training setup that, while not hyperparameters in the traditional sense, influence the experimental procedure.
%
%\paragraph{General Hyperparameter Settings}
%Across all experiments, we observe a set of common hyperparameter choices that are consistent with best practices in reinforcement learning literature. These include:
%
%\begin{itemize}
%	\item \textbf{Total Timesteps:} All experiments are conducted over $100,000$ timesteps, ensuring uniform evaluation across different algorithms and environments.
%	\item \textbf{Learning Rate:} A low learning rate is used, with values ranging from $0.0001$ (for DQN-based methods) to $0.00025$ or $0.0003$ (for policy-based methods like PPO and SAC). These values are chosen to balance stability and convergence speed.
%	\item \textbf{Discount Factor $\gamma$:} Set to $0.99$ across all experiments, ensuring that the agent prioritizes long-term rewards.
%	\item \textbf{Exploration Strategy:} For value-based methods, an $\epsilon$-greedy approach is used, starting at $\epsilon = 1$ and decaying to $\epsilon = 0.01$ over $10\%$ of training timesteps.
%	\item \textbf{Batch Size:} Typically set to $32$ for DQN-based methods, while PPO and SAC use larger values such as $256$ or $1024$, reflecting their different training dynamics.
%	\item \textbf{Target Network Update Frequency:} Set to $1000$ for methods utilizing a target network (e.g., DQN, SAC).
%	\item \textbf{Replay Buffer Size:} Fixed at $10,000$ for DQN-based methods, and increased to $20,000$ for SAC.
%	\item \textbf{Number of Environments:} While DQN and Reinforce use a single environment, PPO employs $8$ parallel environments to improve data efficiency.
%\end{itemize}
%
%These hyperparameter values reflect common heuristics for stable reinforcement learning training, ensuring fair comparisons across algorithms.
%
%
%\paragraph{Algorithm-Specific Considerations}
%Each reinforcement learning algorithm has unique hyperparameter requirements and training considerations. Below, we highlight key aspects for each:
%
%\paragraph{Dueling DQN}
%\begin{itemize}
%	\item Uses a \textbf{target network update frequency} of $1000$ steps to stabilize training.
%	\item Implements an \textbf{$\epsilon$-greedy exploration strategy} with a decay schedule.
%\end{itemize}
%
%\paragraph{PPO (Proximal Policy Optimization)}
%\begin{itemize}
%	\item Utilizes \textbf{multiple epochs per update} (4) and \textbf{minibatches} (4) to process data efficiently.
%	\item Uses \textbf{entropy regularization} ($0.01$) and \textbf{value function coefficient} ($0.5$) to balance exploration and stability.
%	\item Implements \textbf{learning rate annealing} to improve convergence.
%\end{itemize}
%
%\paragraph{SAC (Soft Actor-Critic)}
%\begin{itemize}
%	\item Uses \textbf{automatic entropy tuning} with a target entropy scale of $0.89$.
%	\item Employs a \textbf{separate learning rate for policy and Q-networks} ($0.0003$).
%	\item A larger \textbf{buffer size} ($20,000$) is used to improve sample efficiency.
%\end{itemize}
%
%\paragraph{Reinforce}
%\begin{itemize}
%	\item Uses a simple \textbf{monte-carlo update} approach without replay buffers.
%	\item Employs a \textbf{fixed learning rate} ($0.00025$) with no adaptive mechanisms.
%\end{itemize}
%
%These details are expanded upon in the methodology sections dedicated to each algorithm.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%